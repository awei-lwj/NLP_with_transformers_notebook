{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1F9zVvUxVr8P"
      },
      "outputs": [],
      "source": [
        "#Greedy coding\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model_name = \"gpt2-xl\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "input_txt = \"Transformers are the\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "iterations = []\n",
        "n_steps = 8\n",
        "choices_per_step = 5\n",
        "with torch.no_grad():\n",
        "\tfor _ in range(n_steps):\n",
        "\t\titeration = dict()\n",
        "\t\titeration[\"Input\"] = tokenizer.decode(input_ids[0])\n",
        "\t\toutput = model(input_ids=input_ids)\n",
        "\t\t# Select logits of the first batch and the last token and apply softmax\n",
        "\t\tnext_token_logits = output.logits[0, -1, :]\n",
        "\t\tnext_token_probs = torch.softmax(next_token_logits, dim=-1)\n",
        "\t\tsorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n",
        "\t\t# Store tokens with highest probabilities\n",
        "\t\tfor choice_idx in range(choices_per_step):\n",
        "\t\t\ttoken_id = sorted_ids[choice_idx]\n",
        "\t\t\ttoken_prob = next_token_probs[token_id].cpu().numpy()\n",
        "\t\t\ttoken_choice = ( f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\" )\n",
        "\t\t\titeration[f\"Choice {choice_idx+1}\"] = token_choice\n",
        "\t\t\t# Append predicted next token to input\n",
        "\t\t\tinput_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n",
        "\t\t\titerations.append(iteration)\n",
        "pd.DataFrame(iterations)\n"
      ],
      "metadata": {
        "id": "2c7owTIsYb44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output = model.generate(input_ids, max_new_tokens=n_steps, do_sample=False)\n",
        "print(tokenizer.decode(output[0]))\n"
      ],
      "metadata": {
        "id": "faheFl0iaobh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "0-EZLlFLa4cF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = 128\n",
        "input_txt = \"\"\"In a shocking finding, scientist discovered \\ a herd of unicorns living in a remote, previously unexplored \\ valley, in the Andes Mountains. Even more surprising to the \\ researchers was the fact that the unicorns spoke perfect English.\\n\\n \"\"\"\n",
        "input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n",
        "output_greedy = model.generate(input_ids, max_length=max_length, do_sample=False)\n",
        "print(tokenizer.decode(output_greedy[0]))\n"
      ],
      "metadata": {
        "id": "PRiSbpVFa8kH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Greed用的是直接概率，beam是序列概率用对数"
      ],
      "metadata": {
        "id": "EBU5y8W9cXDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sequence_logprob(model, labels, input_len=0):\n",
        "\twith torch.no_grad():\n",
        "\t\toutput = model(labels)\n",
        "\t\tlog_probs = log_probs_from_logits( output.logits[:, :-1, :], labels[:, 1:])\n",
        "  seq_log_prob = torch.sum(log_probs[:, input_len:])\n",
        "  return seq_log_prob.cpu().numpy()\n"
      ],
      "metadata": {
        "id": "5RIl-8uLcYB9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "top_k固定范围,top_p固定概率大小进行阶段输出"
      ],
      "metadata": {
        "id": "sMMxDis9hTlQ"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HEVSnfPhTQw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
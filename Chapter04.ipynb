{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBYdCsJkejgF"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import get_dataset_config_names\n",
        "xtreme_subsets = get_dataset_config_names(\"xtreme\")\n",
        "print(f\"XTREME has {len(xtreme_subsets)} configurations\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "panx_subsets = [s for s in xtreme_subsets if s.startswith(\"PAN\")]\n",
        "panx_subsets[:3]\n"
      ],
      "metadata": {
        "id": "bDsq6r62fHym"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "load_dataset(\"xtreme\", name=\"PAN-X.de\")\n"
      ],
      "metadata": {
        "id": "0MLRs8i2fagv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "from datasets import DatasetDict\n",
        "langs = [\"de\", \"fr\", \"it\", \"en\"]\n",
        "fracs = [0.629, 0.229, 0.084, 0.059]\n",
        "# Return a DatasetDict if a key doesn't exist\n",
        "panx_ch = defaultdict(DatasetDict)\n",
        "for lang, frac in zip(langs, fracs):\n",
        "\t# Load monolingual corpus\n",
        "\tds = load_dataset(\"xtreme\", name=f\"PAN-X.{lang}\")\n",
        "\t# Shuffle and downsample each split according to spoken proportion\n",
        "\tfor split in ds:\n",
        "\t\tpanx_ch[lang][split] = ( ds[split]\n",
        "\t\t.shuffle(seed=0)\n",
        "\t\t.select(range(int(frac * ds[split].num_rows))))\n"
      ],
      "metadata": {
        "id": "mQhVLrMQfhwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "pd.DataFrame({lang: [panx_ch[lang][\"train\"].num_rows] for lang in langs}, index=[\"Number of training examples\"])\n"
      ],
      "metadata": {
        "id": "BizIxoIGf2JJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "element = panx_ch[\"de\"][\"train\"][0]\n",
        "for key, value in element.items():\n",
        "\tprint(f\"{key}: {value}\")"
      ],
      "metadata": {
        "id": "_xMIPOHugA2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in panx_ch[\"de\"][\"train\"].features.items():\n",
        "\tprint(f\"{key}: {value}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "QbGH5m28gJEZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tags = panx_ch[\"de\"][\"train\"].features[\"ner_tags\"].feature\n",
        "print(tags)"
      ],
      "metadata": {
        "id": "ylilQ-ivgMwx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_tag_names(batch):\n",
        "\treturn {\"ner_tags_str\": [tags.int2str(idx) for idx in batch[\"ner_tags\"]]}\n",
        "panx_de = panx_ch[\"de\"].map(create_tag_names)\n",
        "de_example = panx_de[\"train\"][0]\n",
        "pd.DataFrame([de_example[\"tokens\"], de_example[\"ner_tags_str\"]], ['Tokens', 'Tags'])\n",
        "\n"
      ],
      "metadata": {
        "id": "QFLu-K69gj6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "split2freqs = defaultdict(Counter)\n",
        "for split, dataset in panx_de.items():\n",
        "\tfor row in dataset[\"ner_tags_str\"]:\n",
        "\t\tfor tag in row:\n",
        "\t\t\tif tag.startswith(\"B\"):\n",
        "\t\t\t\ttag_type = tag.split(\"-\")[1]\n",
        "\t\t\t\tsplit2freqs[split][tag_type] += 1\n",
        "\n",
        "pd.DataFrame.from_dict(split2freqs, orient=\"index\")\n"
      ],
      "metadata": {
        "id": "AXQqR8jdgt5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "bert_model_name = \"bert-base-cased\"\n",
        "xlmr_model_name = \"xlm-roberta-base\"\n",
        "bert_tokenizer = AutoTokenizer.from_pretrained(bert_model_name)\n",
        "xlmr_tokenizer = AutoTokenizer.from_pretrained(xlmr_model_name)\n"
      ],
      "metadata": {
        "id": "ft9NdtyahpGz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jack Sparrow loves New York!\"\n",
        "bert_tokens = bert_tokenizer(text).tokens()\n",
        "xlmr_tokens = xlmr_tokenizer(text).tokens()\n"
      ],
      "metadata": {
        "id": "LaPL7L3JhrLR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Jack Sparrow loves New York!\"\n",
        "bert_tokens = bert_tokenizer(text).tokens()\n",
        "xlmr_tokens = xlmr_tokenizer(text).tokens()\n",
        "bert_tokens"
      ],
      "metadata": {
        "id": "baw95IdNh-aS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xlmr_tokens"
      ],
      "metadata": {
        "id": "FzEoG2EliEq5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\".join(xlmr_tokens).replace(u\"\\u2581\", \" \")\n",
        "'<s> Jack Sparrow loves New York!</s>'\n",
        "\n"
      ],
      "metadata": {
        "id": "iKVLNOI3jSPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaConfig, RobertaModel, RobertaPreTrainedModel\n",
        "from transformers.modeling_outputs import TokenClassifierOutput\n",
        "\n",
        "class XLMRobertaForTokenClassification(RobertaPreTrainedModel):\n",
        "    config_class = XLMRobertaConfig\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__(config)\n",
        "        self.num_labels = config.num_labels\n",
        "        # Load model body\n",
        "        self.roberta = RobertaModel(config, add_pooling_layer=False)\n",
        "        # Set up token classification head\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n",
        "        # Load and initialize weights\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, token_type_ids=None, labels=None, **kwargs):\n",
        "        # Use model body to get encoder representations\n",
        "        outputs = self.roberta(input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids, **kwargs)\n",
        "        # Apply classifier to encoder representation\n",
        "        sequence_output = self.dropout(outputs[0])\n",
        "        logits = self.classifier(sequence_output)\n",
        "        # Calculate losses\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "        # Return model output object\n",
        "        return TokenClassifierOutput(loss=loss, logits=logits, hidden_states=outputs.hidden_states, attentions=outputs.attentions)\n"
      ],
      "metadata": {
        "id": "x16Z8qvollvn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "index2tag = {idx: tag for idx, tag in enumerate(tags.names)}\n",
        "tag2index = {tag: idx for idx, tag in enumerate(tags.names)}\n"
      ],
      "metadata": {
        "id": "vBnGwBZQna21"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoConfig\n",
        "xlmr_config = AutoConfig.from_pretrained(xlmr_model_name, num_labels=tags.num_classes, id2label=index2tag, label2id=tag2index)\n"
      ],
      "metadata": {
        "id": "0f7snNwwn50f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "xlmr_model = (XLMRobertaForTokenClassification .from_pretrained(xlmr_model_name, config=xlmr_config) .to(device))\n"
      ],
      "metadata": {
        "id": "EyKp4_ooA9J_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = xlmr_tokenizer.encode(text, return_tensors=\"pt\")\n",
        "pd.DataFrame([xlmr_tokens, input_ids[0].numpy()], index=[\"Tokens\", \"Input IDs\"])\n"
      ],
      "metadata": {
        "id": "baw58qYFBVc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "outputs = xlmr_model(input_ids.to(device)).logits\n",
        "predictions = torch.argmax(outputs, dim=-1)\n",
        "print(f\"Number of tokens in sequence: {len(xlmr_tokens)}\")\n",
        "print(f\"Shape of outputs: {outputs.shape}\")\n"
      ],
      "metadata": {
        "id": "PjOeZqbCC8iW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "pd.DataFrame([xlmr_tokens, preds], index=[\"Tokens\", \"Tags\"])\n"
      ],
      "metadata": {
        "id": "QGHpnGJTDA6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tag_text(text, tags, model, tokenizer):\n",
        "\t# Get tokens with special characters\n",
        "\ttokens = tokenizer(text).tokens()\n",
        "\t# Encode the sequence into IDs\n",
        "\tinput_ids = xlmr_tokenizer(text, return_tensors=\"pt\").input_ids.to(device)\n",
        "\t# Get predictions as distribution over 7 possible classes\n",
        "\toutputs = model(inputs)[0]\n",
        "\t# Take argmax to get most likely class per token\n",
        "\tpredictions = torch.argmax(outputs, dim=2)\n",
        "\t# Convert to DataFrame\n",
        "\tpreds = [tags.names[p] for p in predictions[0].cpu().numpy()]\n",
        "\treturn pd.DataFrame([tokens, preds], index=[\"Tokens\", \"Tags\"])\n"
      ],
      "metadata": {
        "id": "5NlNBqfHErzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words, labels = de_example[\"tokens\"], de_example[\"ner_tags\"]\n"
      ],
      "metadata": {
        "id": "2NR2Yc5ZFvy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_input = xlmr_tokenizer(de_example[\"tokens\"], is_split_into_words=True)\n",
        "tokens = xlmr_tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n",
        "pd.DataFrame([tokens], index=[\"Tokens\"])\n"
      ],
      "metadata": {
        "id": "P0git1guGKOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = tokenized_input.word_ids()\n",
        "pd.DataFrame([tokens, word_ids], index=[\"Tokens\", \"Word IDs\"])\n"
      ],
      "metadata": {
        "id": "lW-8DtxSGk9M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "previous_word_idx = None\n",
        "label_ids = []\n",
        "for word_idx in word_ids:\n",
        "\tif word_idx is None or word_idx == previous_word_idx:\n",
        "\t\tlabel_ids.append(-100)\n",
        "\telif word_idx != previous_word_idx:\n",
        "\t\tlabel_ids.append(labels[word_idx])\n",
        "\tprevious_word_idx = word_idx\n",
        "labels = [index2tag[l] if l != -100 else \"IGN\" for l in label_ids]\n",
        "index = [\"Tokens\", \"Word IDs\", \"Label IDs\", \"Labels\"]\n",
        "pd.DataFrame([tokens, word_ids, label_ids, labels], index=index)\n"
      ],
      "metadata": {
        "id": "hyETobfSG_SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_and_align_labels(examples):\n",
        "\ttokenized_inputs = xlmr_tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n",
        "\tlabels = []\n",
        "\tfor idx, label in enumerate(examples[\"ner_tags\"]):\n",
        "\t\tword_ids = tokenized_inputs.word_ids(batch_index=idx)\n",
        "\t\tprevious_word_idx = None\n",
        "    label_ids = []\n",
        "\t\tfor word_idx in word_ids:\n",
        "\t\t\tif word_idx is None or word_idx == previous_word_idx:\n",
        "\t\t\t\tlabel_ids.append(-100)\n",
        "\t\t\telse:\n",
        "\t\t\t\tlabel_ids.append(label[word_idx])\n",
        "\t\t\tprevious_word_idx = word_idx\n",
        "\t\tlabels.append(label_ids)\n",
        "\ttokenized_inputs[\"labels\"] = labels\n",
        "\treturn tokenized_inputs"
      ],
      "metadata": {
        "id": "QowkudX8IBpp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install seqeval\n",
        "from seqeval.metrics import classification_report\n",
        "y_true = [[\"O\", \"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "y_pred = [[\"O\", \"O\", \"B-MISC\", \"I-MISC\", \"I-MISC\", \"I-MISC\", \"O\"], [\"B-PER\", \"I-PER\", \"O\"]]\n",
        "print(classification_report(y_true, y_pred))"
      ],
      "metadata": {
        "id": "zibkuMPUKHj9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
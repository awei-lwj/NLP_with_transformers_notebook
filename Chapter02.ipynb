{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "DD7GxvOleILE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rbghCmcvTWp6"
      },
      "outputs": [],
      "source": [
        "!pip install datasets\n",
        "from datasets import list_datasets\n",
        "all_datasets = list_datasets()\n",
        "print(f\"There are {len(all_datasets)} datasets currently available on the Hub\")\n",
        "print(f\"The first 10 are: {all_datasets[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "emotions = load_dataset(\"emotion\")\n"
      ],
      "metadata": {
        "id": "Oyh6EqvsfbLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emotions"
      ],
      "metadata": {
        "id": "gMIfiFhbfzwP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds = emotions[\"train\"]\n",
        "\n",
        "train_ds\n"
      ],
      "metadata": {
        "id": "SYRKZXE4f4PA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dataset to dataframe\n",
        "import pandas as pd\n",
        "emotions.set_format(type=\"pandas\")\n",
        "df = emotions[\"train\"][:]\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "yn5_zfVbgEqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def label_int2str(row):\n",
        "\treturn emotions[\"train\"].features[\"label\"].int2str(row)\n",
        "df[\"label_name\"] = df[\"label\"].apply(label_int2str)\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "XSxbEMHkgPfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "这个推文长度分布倒是要好好看一下怎么画的"
      ],
      "metadata": {
        "id": "jTNNmxnYGTPC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"Words Per Tweet\"] = df[\"text\"].str.split().apply(len)\n",
        "df.boxplot(\"Words Per Tweet\", by=\"label_name\", grid=False, showfliers=False, color=\"black\")\n",
        "plt.suptitle(\"\")\n",
        "plt.xlabel(\"\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "B8JzE94ZGXQQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "词汇标记化\n",
        "像DistilBERT这样的转化器模型不能接收原始字符串作为输入。 相反，他们假设文本已被标记化并被编码为数字向量。 符号化是将字符串分解为模型中使用的原子单元的步骤。 有几种标记化策略可以采用，而且通常从语料库中学习到最佳的单词拆分成子单元的方法。 在研究用于DistilBERT的标记器之前，让我们考虑两个极端情况。 字符和词的标记化。"
      ],
      "metadata": {
        "id": "XZz5VYcbI82U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotions.reset_format()"
      ],
      "metadata": {
        "id": "3qFT38HJJEtq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Tokenizing text is a core task of NLP.\"\n",
        "tokenized_text = list(text)\n",
        "print(tokenized_text)\n",
        "['T', 'o', 'k', 'e', 'n', 'i', 'z', 'i', 'n', 'g', ' ', 't', 'e', 'x', 't', ' ', 'i', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'e', ' ', 't', 'a', 's', 'k', ' ', 'o', 'f', ' ', 'N', 'L', 'P', '.']\n"
      ],
      "metadata": {
        "id": "AnbxatWhJHiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "token2idx = {ch: idx for idx, ch in enumerate(sorted(set(tokenized_text)))}\n",
        "print(token2idx)\n",
        "{' ': 0, '.': 1, 'L': 2, 'N': 3, 'P': 4, 'T': 5, 'a': 6, 'c': 7, 'e': 8, 'f': 9, 'g': 10, 'i': 11, 'k': 12, 'n': 13, 'o': 14, 'r': 15, 's': 16, 't': 17, 'x': 18, 'z': 19}\n"
      ],
      "metadata": {
        "id": "_bugB4ZCJfOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = [token2idx[token] for token in tokenized_text]\n",
        "print(input_ids)\n",
        "\n",
        "[5, 14, 12, 8, 13, 11, 19, 11, 13, 10, 0, 17, 8, 18, 17, 0, 11, 16, 0, 6, 0, 7, 14, 15, 8, 0, 17, 6, 16, 12, 0, 14, 9, 0, 3, 2, 4, 1]\n"
      ],
      "metadata": {
        "id": "DzKdRETbJKzX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df = pd.DataFrame( {\"Name\": [\"Bumblebee\", \"Optimus Prime\", \"Megatron\"], \"Label ID\": [0,1,2]})\n",
        "\n",
        "categorical_df\n"
      ],
      "metadata": {
        "id": "rC1uXbxPJidR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "input_ids = torch.tensor(input_ids)\n",
        "one_hot_encodings = F.one_hot(input_ids, num_classes=len(token2idx))\n",
        "\n",
        "\n",
        "\n",
        "one_hot_encodings.shape\n",
        "torch.Size([38, 20])\n"
      ],
      "metadata": {
        "id": "8xDmwSdpJlay"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "为了对整个语料库进行标记，我们将使用DatasetDict对象的map（）方法。 在本书中，我们会多次遇到这种方法，因为它提供了一种方便的方法，可以对数据集中的每个元素应用处理函数。 我们很快就会看到，map()方法也可以用来创建新的行和列。"
      ],
      "metadata": {
        "id": "q6GC4NRlLMRk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModel\n",
        "model_ckpt = \"distilbert-base-uncased\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = AutoModel.from_pretrained(model_ckpt).to(device)\n"
      ],
      "metadata": {
        "id": "gX17p-50MCq2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TFAutoModel\n",
        "tf_model = TFAutoModel.from_pretrained(model_ckpt)\n"
      ],
      "metadata": {
        "id": "DM9gsB97MI1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_xlmr = TFAutoModel.from_pretrained(\"xlm-roberta-base\")\n"
      ],
      "metadata": {
        "id": "ZnMAv8sFMNJq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"this is a test\"\n",
        "inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "print(f\"Input tensor shape: {inputs['input_ids'].size()}\")\n"
      ],
      "metadata": {
        "id": "_ApQdrunMP31"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}